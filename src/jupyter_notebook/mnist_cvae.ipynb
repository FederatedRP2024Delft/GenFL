{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from src.utils import get_dataset\n",
    "from src.vae.mnist_vae import ConditionalVae\n",
    "import matplotlib.pyplot as plt\n",
    "from src.impute import impute_cvae_naive\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from src.image_classifier.exq_net_v1 import ExquisiteNetV1\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T08:35:48.609327Z",
     "start_time": "2024-05-29T08:35:46.885197Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sampling'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_dataset\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvae\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmnist_vae\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConditionalVae\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "File \u001B[0;32m~/projects/Federated-Learning-PyTorch/src/utils.py:12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributions\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnormal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Normal\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorcheval\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FrechetInceptionDistance\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msampling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dataset\u001B[39m(args):\n\u001B[1;32m     16\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns train and test datasets and a user group which is a dict where\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    the keys are the user index and the values are the corresponding data for\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m    each of those users.\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sampling'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:34:52.946716Z",
     "start_time": "2024-05-27T10:34:40.207287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# binarize the data\n",
    "class args:\n",
    "    def __init__(self):\n",
    "        self.num_channels = 1\n",
    "        self.iid = 1\n",
    "        self.num_classes = 10\n",
    "        self.num_users = 10\n",
    "        self.dataset = 'mnist'\n",
    "\n",
    "training_data, testing_data, user_groups = get_dataset(args())"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:34:55.174479Z",
     "start_time": "2024-05-27T10:34:54.989272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.imshow(training_data[0][0][0], cmap='gray')\n",
    "\n",
    "print(training_data[0][0][0])"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = \"cvae\"\n",
    "dataset = \"mnist\"\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_path = f\"../../models/local_{model}_{dataset}_{batch_size}_{epochs}_{learning_rate}.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    cvae_model = torch.load(model_path)\n",
    "else:\n",
    "    cvae = ConditionalVae(dim_encoding=3).to(device)\n",
    "\n",
    "    # try with model sigma\n",
    "    cvae_model, vae_loss_li, kl_loss_li, reg_loss_li = cvae.train_model(\n",
    "        training_data=training_data,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    torch.save(cvae_model, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T10:38:02.940259Z",
     "start_time": "2024-05-27T10:35:05.522305Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:38:20.069130Z",
     "start_time": "2024-05-27T10:38:09.855484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate synthetic data and 70000 real testing dataset\n",
    "gen_train_dataset = impute_cvae_naive(k=60000, trained_cvae = cvae_model, initial_dataset = torch.tensor([]))\n",
    "final_testing_data = torch.utils.data.ConcatDataset([training_data, testing_data])\n",
    "print(final_testing_data)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:17.930980Z",
     "start_time": "2024-05-27T10:38:21.938156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train classifier on gen data and test on entire 70000 real dataset\n",
    "model = \"exq_v1\"\n",
    "dataset = \"mnist\"\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "\n",
    "train_loader= DataLoader(gen_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(final_testing_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model_path = f\"../../models/local_{model}_{dataset}_{batch_size}_{epochs}_{learning_rate}.pt\"\n",
    "\n",
    "classifier = ExquisiteNetV1(class_num=10, img_channels=1).to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Number of epochs to train the model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "f1_scores = []\n",
    "cas_scores = []\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0.0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = classifier(data)\n",
    "        pred_labels.append(output.argmax(dim=1))\n",
    "        actual_labels.append(target)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        test_pred_labels = []\n",
    "        test_actual_labels = []\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = classifier(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            test_pred_labels.append(output.argmax(dim=1))\n",
    "            test_actual_labels.append(target)\n",
    "             # Compare with actual classes\n",
    "            total_predictions += output.argmax(dim=1).size(0)\n",
    "            # correct_predictions += (predicted == labels).sum().item()\n",
    "            correct_predictions += (output.argmax(dim=1) == target).sum().item()\n",
    "            \n",
    "    # Compute average test loss\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Calculate F1 score for the test data\n",
    "    test_pred_labels = torch.cat(test_pred_labels).to('cpu').numpy()\n",
    "    test_actual_labels = torch.cat(test_actual_labels).to('cpu').numpy()\n",
    "    test_f1_score = f1_score(test_actual_labels, test_pred_labels, average='macro')\n",
    "    f1_scores.append(test_f1_score)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    cas_scores.append(accuracy)\n",
    "\n",
    "    print(f'Accuracy: {accuracy * 100}%')\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t Test Loss: {:.6f} \\tF1 Test Macro: {:.6f}'.format(\n",
    "        epoch + 1,\n",
    "        train_loss,\n",
    "        test_loss,\n",
    "        test_f1_score\n",
    "    ))\n",
    "  \n",
    "    print(\"Train losses: \", train_losses)\n",
    "    print(\"Test losses: \", test_losses)\n",
    "    print(\"F1 scores: \", f1_scores)\n",
    "    print(\"CAS scores: \", cas_scores)\n",
    "\n",
    "    # torch.save(classifier, model_path)"
   ],
   "execution_count": 6,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
