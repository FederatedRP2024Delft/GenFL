{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-20T19:16:06.919347Z",
     "start_time": "2024-05-20T19:16:06.916307Z"
    }
   },
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "from torch import cuda, device, Tensor, save, load, stack, zeros, vstack, reshape, squeeze\n",
    "from src.plots import plot_vae_training_result, plot_image, plot_image_label_two\n",
    "from src.vae.mnist_vae import ConditionalVae\n",
    "from src.image_classifier.image_classifier import MNISTClassifier\n",
    "from src.utils import frechet_inception_distance\n",
    "\n",
    "device = device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:16:08.551146Z",
     "start_time": "2024-05-20T19:16:08.518305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_data = torchvision.datasets.FashionMNIST(root='../../data/FMNIST_train', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "testing_data = torchvision.datasets.FashionMNIST(root='../../data/FMNIST_test', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "print(training_data)\n",
    "print(testing_data)\n",
    "\n",
    "print(training_data.data.shape)\n",
    "print(training_data.targets.shape)\n",
    "\n",
    "input = training_data.data[:60000]\n",
    "labels = training_data.targets[:60000]"
   ],
   "id": "16ee1ea9b79842dc",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:17:57.456977Z",
     "start_time": "2024-05-20T19:16:58.204274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parameters\n",
    "model = \"cvae\"\n",
    "dataset = \"fmnist\"\n",
    "batch_size = 64\n",
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "model_path = f\"../../models/{model}_{dataset}_{batch_size}_{epoch}_{learning_rate}.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    vae = load(model_path)\n",
    "else:\n",
    "    vae = ConditionalVae(dim_encoding=3).to(device)\n",
    "\n",
    "    # try with model sigma\n",
    "    vae_model, vae_loss_li, kl_loss_li = vae.train_model(\n",
    "        training_data=training_data,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epoch,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    save(vae, model_path)\n",
    "    \n",
    "    # move tensors to cpu before converting to np array\n",
    "    np_kl_loss_li = []\n",
    "    \n",
    "    for output in kl_loss_li:\n",
    "        if isinstance(output, Tensor):\n",
    "            np_kl_loss_li.append(output.cpu().detach().numpy())\n",
    "    \n",
    "    # plot results\n",
    "    plot_vae_training_result(\n",
    "        input=input,\n",
    "        labels=labels,\n",
    "        vae_model=vae_model,\n",
    "        vae_loss_li=vae_loss_li,\n",
    "        kl_loss_li=np_kl_loss_li\n",
    "    )"
   ],
   "id": "415ddc3205ecd96b",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:18:02.647146Z",
     "start_time": "2024-05-20T19:18:01.874696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    images = vae.generate_data(n_samples=5, target_label=i)\n",
    "    plot_image(images)"
   ],
   "id": "8c38d9f63fe4cdfc",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:18:11.214099Z",
     "start_time": "2024-05-20T19:18:08.811045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train classifier for performance evaluation\n",
    "model = \"classifier\"\n",
    "dataset = \"fmnist\"\n",
    "batch_size = 64\n",
    "epoch = 10\n",
    "\n",
    "classifier_path = f\"../../models/{model}_{dataset}_{batch_size}_{epoch}.pt\"\n",
    "\n",
    "if os.path.exists(classifier_path):\n",
    "    classifier = load(classifier_path)\n",
    "else:\n",
    "    classifier = MNISTClassifier(input_size=784, num_classes=10)\n",
    "    classifier.train_model(training_data, batch_size=batch_size, epochs=epoch)\n",
    "accuracy = classifier.test_model(testing_data)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "save(classifier, classifier_path)"
   ],
   "id": "1296469b8fed46fe",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:18:15.755164Z",
     "start_time": "2024-05-20T19:18:14.350601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate data for testing on classifier\n",
    "# this one stays the same\n",
    "data_count = 10000\n",
    "ratios = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label_idx, ratio in enumerate(ratios):\n",
    "    num_samples_to_generate = int(data_count * ratio)\n",
    "    images.append(vae.generate_data(n_samples=num_samples_to_generate, target_label=label_idx).cpu().detach())\n",
    "    \n",
    "    label = zeros((num_samples_to_generate, 10), device=device)\n",
    "    label[:, label_idx] = 1\n",
    "    labels.append(label.cpu().detach())\n",
    "\n",
    "final_images = vstack(images)\n",
    "final_labels = vstack(labels)\n",
    "\n",
    "assert final_images.shape[0] == final_labels.shape[0]\n",
    "\n",
    "accuracy = classifier.test_model_syn_img_label(final_images, final_labels)\n",
    "print(\"Accuracy: \", accuracy)"
   ],
   "id": "b06c169395541bf6",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:12:04.906433Z",
     "start_time": "2024-05-20T19:12:00.297779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine FID\n",
    "# generate 500 images\n",
    "# normalizing necessary to make pixels in [0, 1] range for FID\n",
    "\n",
    "testing_data = torchvision.datasets.MNIST(root='../../data/FMNIST_test', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "num_data = 100\n",
    "\n",
    "input = testing_data.data[:num_data] / 255.0\n",
    "syn_input = vae.generate_data(n_samples=num_data)\n",
    "\n",
    "input_rgb = input.view(-1, 1, 28, 28).repeat(1, 3, 1, 1)\n",
    "syn_input_rgb = syn_input.view(-1, 1, 28, 28).repeat(1, 3, 1, 1)\n",
    "\n",
    "# compute FID score\n",
    "fid_score = frechet_inception_distance(input_rgb, syn_input_rgb)\n",
    "print(\"Frechet Inception Distance: \", fid_score)"
   ],
   "id": "8facfceca09d32d",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:18:21.666525Z",
     "start_time": "2024-05-20T19:18:17.824260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test baseline as IID data\n",
    "num_users = [3, 5, 10]\n",
    "for num_user in num_users:\n",
    "    model_state_dict_path = f\"../../models/federated_cvae_fmnist_1_1.0_20_2_{num_user}.pt\"\n",
    "    loaded_state_dict = load(model_state_dict_path)\n",
    "    global_model = ConditionalVae(dim_encoding=3)\n",
    "    global_model.load_state_dict(loaded_state_dict)\n",
    "        \n",
    "    data_count = 10000\n",
    "    ratios = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label_idx, ratio in enumerate(ratios):\n",
    "        num_samples_to_generate = int(data_count * ratio)\n",
    "        images.append(global_model.generate_data(n_samples=num_samples_to_generate, target_label=label_idx).cpu().detach())\n",
    "        \n",
    "        label = zeros((num_samples_to_generate, 10), device=device)\n",
    "        label[:, label_idx] = 1\n",
    "        labels.append(label.cpu().detach())\n",
    "    \n",
    "    final_images = vstack(images)\n",
    "    final_labels = vstack(labels)\n",
    "    \n",
    "    assert final_images.shape[0] == final_labels.shape[0]\n",
    "    \n",
    "    accuracy = classifier.test_model_syn_img_label(final_images, final_labels)\n",
    "    print(f\"IID data num users {num_user} accuracy: \", accuracy)\n",
    "    # \n",
    "    # for i in range(10):\n",
    "    #     images = global_model.generate_data(n_samples=5, target_label=i)\n",
    "    #     plot_image(images)"
   ],
   "id": "3c7519d8d74dd91d",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:13:36.632200Z",
     "start_time": "2024-05-20T19:12:59.799557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dirichlet_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "num_users = [3, 5, 10]\n",
    "\n",
    "for num_user in num_users:\n",
    "    for dirichlet_ratio in dirichlet_ratios:\n",
    "        model_state_dict_path = f\"../../models/federated_cvae_fmnist_2_{dirichlet_ratio}_20_2_{num_user}.pt\"\n",
    "        loaded_state_dict = load(model_state_dict_path)\n",
    "        global_model = ConditionalVae(dim_encoding=3)\n",
    "        global_model.load_state_dict(loaded_state_dict)\n",
    "            \n",
    "        data_count = 10000\n",
    "        ratios = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for label_idx, ratio in enumerate(ratios):\n",
    "            num_samples_to_generate = int(data_count * ratio)\n",
    "            images.append(global_model.generate_data(n_samples=num_samples_to_generate, target_label=label_idx).cpu().detach())\n",
    "            \n",
    "            label = zeros((num_samples_to_generate, 10), device=device)\n",
    "            label[:, label_idx] = 1\n",
    "            labels.append(label.cpu().detach())\n",
    "        \n",
    "        final_images = vstack(images)\n",
    "        final_labels = vstack(labels)\n",
    "        \n",
    "        assert final_images.shape[0] == final_labels.shape[0]\n",
    "        \n",
    "        accuracy = classifier.test_model_syn_img_label(final_images, final_labels)\n",
    "        print(f\"Num user {num_user}, Dirichlet beta {dirichlet_ratio} accuracy: \", accuracy)\n",
    "        \n",
    "        if num_user == 10 and dirichlet_ratio == 0.1:\n",
    "            for i in range(10):\n",
    "                images = global_model.generate_data(n_samples=5, target_label=i)\n",
    "                plot_image(images)"
   ],
   "id": "16e2c06b332ebf9a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:18:25.565204Z",
     "start_time": "2024-05-20T19:18:25.136648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train classifier on generated images\n",
    "data_count = 60000\n",
    "ratios = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label_idx, ratio in enumerate(ratios):\n",
    "    num_samples_to_generate = int(data_count * ratio)\n",
    "    images.append(vae.generate_data(n_samples=num_samples_to_generate, target_label=label_idx).cpu().detach())\n",
    "    label = zeros((num_samples_to_generate, 10), device=device)\n",
    "    label[:, label_idx] = 1\n",
    "    labels.append(label.cpu().detach())\n",
    "final_images = vstack(images)\n",
    "final_labels = vstack(labels)\n",
    "\n",
    "training_data = torchvision.datasets.FashionMNIST(root='../../data/FMNIST_train', train=True, download=True,\n",
    "                                                  transform=torchvision.transforms.ToTensor())\n",
    "training_data.data = squeeze(final_images, dim=1)\n",
    "training_data.targets = final_labels.argmax(dim=1)\n",
    "\n",
    "print(training_data.data.shape)\n",
    "print(training_data.targets.shape)"
   ],
   "id": "b1f39a425969353d",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:20:43.135539Z",
     "start_time": "2024-05-20T19:19:29.564771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train classifier for performance evaluation\n",
    "batch_size = 32\n",
    "epoch = 10\n",
    "\n",
    "classifier = MNISTClassifier(input_size=784, num_classes=10)\n",
    "classifier.train_model(training_data, batch_size=batch_size, epochs=epoch)\n",
    "\n",
    "testing_data = torchvision.datasets.FashionMNIST(root='../../data/FMNIST_test', train=False, download=True,\n",
    "                                                 transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "accuracy = classifier.test_model(testing_data)\n",
    "print(\"Test accuracy: \", accuracy)"
   ],
   "id": "ef580531e92862a8",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "66144af011f2059d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
